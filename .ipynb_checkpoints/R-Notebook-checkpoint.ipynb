{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ca144519",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Time Series Analysis and Forecasting Project\"\n",
    "author: \"Feroldi Francesco\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22129bf4-1df2-4f91-9e7b-4ae718f7af63",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Caricamento pacchetto: 'dplyr'\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "\n",
      "Caricamento pacchetto: 'lubridate'\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:base':\n",
      "\n",
      "    date, intersect, setdiff, union\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'performanceEstimation' è stato creato con R versione 4.2.3\"\n",
      "Warning message:\n",
      "\"il pacchetto 'PerformanceAnalytics' è stato creato con R versione 4.2.3\"\n",
      "Caricamento del pacchetto richiesto: xts\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'xts' è stato creato con R versione 4.2.3\"\n",
      "Caricamento del pacchetto richiesto: zoo\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'zoo' è stato creato con R versione 4.2.3\"\n",
      "\n",
      "Caricamento pacchetto: 'zoo'\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:base':\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "\n",
      "################################### WARNING ###################################\n",
      "# We noticed you have dplyr installed. The dplyr lag() function breaks how    #\n",
      "# base R's lag() function is supposed to work, which breaks lag(my_xts).      #\n",
      "#                                                                             #\n",
      "# Calls to lag(my_xts) that you enter or source() into this session won't     #\n",
      "# work correctly.                                                             #\n",
      "#                                                                             #\n",
      "# All package code is unaffected because it is protected by the R namespace   #\n",
      "# mechanism.                                                                  #\n",
      "#                                                                             #\n",
      "# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n",
      "#                                                                             #\n",
      "# You can use stats::lag() to make sure you're not using dplyr::lag(), or you #\n",
      "# can add conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop   #\n",
      "# dplyr from breaking base R's lag() function.                                #\n",
      "################################### WARNING ###################################\n",
      "\n",
      "\n",
      "Caricamento pacchetto: 'xts'\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:dplyr':\n",
      "\n",
      "    first, last\n",
      "\n",
      "\n",
      "\n",
      "Caricamento pacchetto: 'PerformanceAnalytics'\n",
      "\n",
      "\n",
      "Il seguente oggetto è mascherato da 'package:graphics':\n",
      "\n",
      "    legend\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'Hmisc' è stato creato con R versione 4.2.3\"\n",
      "\n",
      "Caricamento pacchetto: 'Hmisc'\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:dplyr':\n",
      "\n",
      "    src, summarize\n",
      "\n",
      "\n",
      "I seguenti oggetti sono mascherati da 'package:base':\n",
      "\n",
      "    format.pval, units\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'corrplot' è stato creato con R versione 4.2.3\"\n",
      "corrplot 0.92 loaded\n",
      "\n",
      "Warning message:\n",
      "\"il pacchetto 'forecast' è stato creato con R versione 4.2.3\"\n"
     ]
    }
   ],
   "source": [
    "# packages to install\n",
    "# install.packages(\"performanceEstimation\")\n",
    "# install.packages(\"fpp3\")\n",
    "# install.packages(\"knitr\")\n",
    "# install.packages(\"vars\")\n",
    "\n",
    "library(dplyr)\n",
    "library(lubridate)\n",
    "library(ggplot2)\n",
    "library(ggplot2)\n",
    "library(performanceEstimation)\n",
    "library(PerformanceAnalytics)\n",
    "library(Hmisc)\n",
    "library(corrplot)\n",
    "library(forecast)\n",
    "library(knitr)\n",
    "library(vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301f33d",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "This project is dedicated to the comprehensive analysis of the collective behavior of a network of electricity consumers by making inference on the aggregated data representing the average user. \n",
    "\n",
    "While the Python component of the project focuses on predicting the behavior of individual consumers, the goal for the following notebook is to unravel the impact of heterogeneous variables on the consumption profile of the network.\n",
    "To do so we will operate under the following assumptions:\n",
    "\n",
    "1) **Representativeness of Data**: We presume that, with the exception of the initial months when clients were joining the network, the data accurately reflects consumption patterns across the entire network;\n",
    "2) **Client Consumption Scope**: It is assumed that clients exhibit no energy consumption beyond what is documented in the provided dataset;\n",
    "3) **Missing Values Attribution**: Missing values are interpreted as a consequence of communication issues between smart meters and the operations center, rather than indicating zero consumption by the client.\n",
    "4) **Downward Trend Causes**: The downward trend identified through yearly seasonal decomposition is attributed to variations in consumer behavior, such as the adoption of greener technologies (e.g., solar panels, LED light bulbs, improved water heaters), rather than being solely influenced by weather conditions.;\n",
    "\n",
    "# STEP 1: IMPORT AND ORGANIZE DATA FROM FS\n",
    "The dataset was distributed across various CSV files. However, for the purposes of this analysis, only four files held pertinent information for inclusion in the case study:\n",
    "\n",
    "* `daily_dataset.csv` Encompassing the daily consumption profiles of users within the distribution network;\n",
    "* `information_households.csv` Containing identifiers for households and their tariff plan types, either Std (Standard) or ToU (Time of Use);\n",
    "* `uk_bank_holiday.csv` Listing all dates designated as UK bank holidays;\n",
    "* `weather_daily_darksky.csv` Detailing the daily weather conditions in London throughout the analysis period.\n",
    "\n",
    "Once the import process was completed it was a matter of merging the tables of the dataset using the primary and foreign keys of one another.s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c815f87",
   "metadata": {
    "scrolled": true,
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "daily_dataset <- \".\\\\archive\\\\daily_dataset.csv\"\n",
    "informations_households <- \".\\\\archive\\\\informations_households.csv\"\n",
    "uk_bank_holidays <- \".\\\\archive\\\\uk_bank_holidays.csv\"\n",
    "weather_daily_darksky <- \".\\\\archive\\\\weather_daily_darksky.csv\"\n",
    "\n",
    "# Import the CSV files\n",
    "daily_dataset <- read.csv(daily_dataset)\n",
    "informations_households <- read.csv(informations_households)\n",
    "uk_bank_holidays <- read.csv(uk_bank_holidays)\n",
    "weather_daily_darksky <- read.csv(weather_daily_darksky)\n",
    "\n",
    "# Augment daily_dataset: flag UK holiday days, add DOW and DOY columns,\n",
    "daily_dataset.augmented <- daily_dataset %>%\n",
    "  mutate(\n",
    "      isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),\n",
    "      day = as.Date(day, format = \"%Y-%m-%d\"),\n",
    "      dayOfWeek = wday(day, week_start=1),\n",
    "      dayOfYear = as.numeric(strftime(day, format = \"%j\")),\n",
    "      consumption = energy_sum\n",
    "  )\n",
    "\n",
    "# Augment weather_daily_darksky: add sunlight hours column, correct format of day column and remove duplicates\n",
    "weather_daily_darksky <- weather_daily_darksky %>%\n",
    "    mutate(\n",
    "        sunlightHours = difftime(sunsetTime, sunriseTime, units = \"hours\"),\n",
    "        weather_condition = icon\n",
    "    )\n",
    "weather_daily_darksky$day <- as.Date(weather_daily_darksky$time)\n",
    "weather_daily_darksky <- weather_daily_darksky[!duplicated(weather_daily_darksky$day), ]\n",
    "\n",
    "# Merge datasets and select only the useful columns\n",
    "daily_dataset.merged <- daily_dataset.augmented %>%\n",
    "    inner_join(informations_households, by = \"LCLid\") %>%\n",
    "    inner_join(weather_daily_darksky, by = c(\"day\" = \"day\")) %>%\n",
    "    dplyr::select(\n",
    "        LCLid,\n",
    "        day,\n",
    "        consumption,\n",
    "        isUkHoliday,\n",
    "        dayOfWeek,\n",
    "        dayOfYear,\n",
    "        stdorToU,\n",
    "        weather_condition,\n",
    "        cloudCover,\n",
    "        windBearing,\n",
    "        windSpeed,\n",
    "        pressure,\n",
    "        visibility,\n",
    "        humidity,\n",
    "        precipType,\n",
    "        uvIndex,\n",
    "        sunlightHours,\n",
    "        temperatureMin,\n",
    "        temperatureMax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac01f64",
   "metadata": {},
   "source": [
    "# STEP 2: DATA EXPLORATION, DATA CLEANING AND DATA TRANSFORMATION\n",
    "\n",
    "## 2.1: Data frequency analysis\n",
    "In this phase, our focus shifts to the exploration, cleaning, and transformation of the dataset. This critical process is undertaken with the explicit goal of ensuring the data's reliability and enhancing its utility for subsequent analyses.\n",
    "\n",
    "To gain insight into the temporal distribution of data points, a frequency table is generated across different days in the `daily_dataset`. The resulting frequency data is visualized using a line plot. \n",
    "\n",
    "Notably, a discernible pattern emerges, revealing a scarcity of available data in the initial months of the program. To address this observation, a threshold is introduced representing 75% of the maximum frequency. This threshold serves as a filter, effectively excluding days with lower data frequency from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f08847",
   "metadata": {
    "fig.cap": "Daily Dataset density distribution",
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "freq_table <- table(daily_dataset$day)\n",
    "freq_data <- as.data.frame(table(daily_dataset$day))\n",
    "freq_data$day <- as.Date(names(freq_table))\n",
    "\n",
    "threshold <- 0.75 * max(freq_data$Freq)\n",
    "\n",
    "ggplot(freq_data, aes(x = day, y = Freq)) +\n",
    "  geom_line() +\n",
    "  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"red\") +\n",
    "  labs(title = \"Daily Dataset Frequency Plot\", x = \"Day\", y = \"Frequency\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b2f46",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# filtering values whose frequency is lower than threshold\n",
    "daily_dataset.filtered <- daily_dataset.merged[daily_dataset.merged$day %in% freq_data$day[freq_data$Freq >= threshold], ]\n",
    "\n",
    "unfiltered_data_points.mln <- round(nrow(daily_dataset.merged) / 1000000, 1)\n",
    "filtered_data_points.mln <- round(nrow(daily_dataset.filtered) / 1000000, 1)\n",
    "\n",
    "cat(\"Unfiltered Data Points (in millions):\", unfiltered_data_points.mln, \"\\n\")\n",
    "cat(\"Filtered Data Points (in millions):\", filtered_data_points.mln, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcfa856",
   "metadata": {},
   "source": [
    "The impact of this filtration is quantified with unfiltered data points totaling 3.5 million and filtered data points and representative of higher-frequency occurrences, amounting to 3.2 million. \n",
    "\n",
    "## 2.2: missing values imputation\n",
    "Continuing our data refinement journey, the next step involves an examination of missing values within the filtered dataset which sets the stage for targeted imputation strategies and ensures the integrity of our data for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b311e4e",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# identifying missing values\n",
    "\n",
    "summary(daily_dataset.filtered)\n",
    "\n",
    "consumption_missing_values <- sum(is.na(daily_dataset.filtered$consumption))\n",
    "cloudCover_missing_values <- sum(is.na(daily_dataset.filtered$cloudCover))\n",
    "uvIndex_missing_values <- sum(is.na(daily_dataset.filtered$uvIndex))\n",
    "\n",
    "cat(\"Consumption Missing Values:\", consumption_missing_values, \"\\n\")\n",
    "cat(\"Cloud Cover Missing Values:\", cloudCover_missing_values, \"\\n\")\n",
    "cat(\"UV Index Missing Values:\", uvIndex_missing_values, \"\\n\")\n",
    "\n",
    "cc_uv_missing_values.date <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$cloudCover) & is.na(daily_dataset.filtered$uvIndex), ]$day) # nolint: line_length_linter.\n",
    "cat(\"Unique Dates with Both Cloud Cover and UV Index Missing Values:\", format(cc_uv_missing_values.date, format = \"%d %B %Y\"), \"\\n\")\n",
    "\n",
    "consumption_missing_values.dates.num <- length(unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$day)) # nolint: line_length_linter.\n",
    "cat(\"Number of Unique Dates with Consumption Missing Values:\", consumption_missing_values.dates.num, \"\\n\")\n",
    "\n",
    "values_cc_uv_missing_values.dates <- sum(daily_dataset.filtered$day == as.Date(cc_uv_missing_values.date))\n",
    "cat(\"Number of values for the [\", format(cc_uv_missing_values.date, format = \"%d %B %Y\"), \"]: \", values_cc_uv_missing_values.dates, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a479891",
   "metadata": {},
   "source": [
    "In our examination of the dataset, a small number of null values were identified across three distinct columns:\n",
    "\n",
    "* \\[9\\] entries in the `Consumption` column\n",
    "* \\[5099\\] entries in both the `Cloud Coverage` index and `Ultraviolet Index` columns\n",
    "\n",
    "The null values in the Consumption column are scattered across \\[9\\] different days. Conversely, the missing values in the Cloud Coverage Index and Ultraviolet Index columns represent the entire dataset for the 1st of January 2014. This absence of values is attributed to the original data source, \"weather_daily_darksky,\" where each row corresponds to the weather conditions for a single day.\n",
    "\n",
    "Given the nature of this dataset, employing basic data imputation techniques might lead to misleading results, particularly for variables strongly influenced by weather conditions. A change in weather conditions can significantly impact these variables. Therefore, a more sophisticated approach is suggested.\n",
    "\n",
    "Considering the complexity of our data, the K-nearest neighbors (KNN) data imputation function emerges as a promising solution. This method takes into account the proximity of data points in a multidimensional space, making it well-suited for scenarios where variables are closely tied to contextual factors such as weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8927fe",
   "metadata": {
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cloud Cover and UV Index Imputation of Missing Values\n",
    "\n",
    "# Impute missing values for UV Index and Cloud Cover in the 'weather_daily_darksky' table\n",
    "weather_daily_darksky$sunlightHours_numeric <- as.numeric(weather_daily_darksky$sunlightHours)\n",
    "\n",
    "# Define features for imputation in 'weather_daily_darksky'\n",
    "features <- c(\"temperatureMax\", \"temperatureMin\", \"windBearing\", \"dewPoint\", \"windSpeed\", \"pressure\", \"visibility\", \"humidity\", \"sunlightHours_numeric\", \"x\")\n",
    "\n",
    "# Impute values\n",
    "weather_daily_darksky$x <- as.numeric(weather_daily_darksky$uvIndex)\n",
    "weather_daily_darksky$uvIndex <- knnImp(weather_daily_darksky[, features], k = 10)$x\n",
    "weather_daily_darksky$x <- as.numeric(weather_daily_darksky$cloudCover)\n",
    "weather_daily_darksky$cloudCover <- knnImp(weather_daily_darksky[, features], k = 10)$x\n",
    "\n",
    "# Re-Merge and select relevant columns in 'daily_dataset.filtered'\n",
    "daily_dataset.filtered <- daily_dataset.filtered[, !(names(daily_dataset.filtered) %in% setdiff(colnames(weather_daily_darksky), \"day\"))]\n",
    "daily_dataset.filtered <- daily_dataset.filtered %>%\n",
    "  inner_join(weather_daily_darksky, by = c(\"day\" = \"day\")) %>%\n",
    "  dplyr::select(\n",
    "    LCLid,\n",
    "    day,\n",
    "    consumption,\n",
    "    isUkHoliday,\n",
    "    dayOfWeek,\n",
    "    dayOfYear,\n",
    "    stdorToU,\n",
    "    weather_condition,\n",
    "    cloudCover,\n",
    "    windBearing,\n",
    "    windSpeed,\n",
    "    pressure,\n",
    "    visibility,\n",
    "    humidity,\n",
    "    precipType,\n",
    "    uvIndex,\n",
    "    sunlightHours,\n",
    "    temperatureMin,\n",
    "    temperatureMax,\n",
    "    sunlightHours_numeric\n",
    "  )\n",
    "\n",
    "daily_dataset.filtered$sunlightHours <- as.numeric(daily_dataset.filtered$sunlightHours)\n",
    "\n",
    "# Define the features for imputation\n",
    "features <- c(\"dayOfYear\", \"dayOfWeek\", \"temperatureMax\", \"temperatureMin\", \"windBearing\", \"windSpeed\", \"pressure\", \"visibility\", \"humidity\", \"sunlightHours\", \"x\", \"uvIndex\", \"cloudCover\")\n",
    "\n",
    "# imputate on a per-user basis\n",
    "LCLid.list <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$LCLid)\n",
    "\n",
    "# Iterate through users for consumption imputation\n",
    "daily_dataset.filtered$x <- as.numeric(daily_dataset.filtered$consumption)\n",
    "for(lcl in LCLid.list) {\n",
    "  lcl_rows = daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ]\n",
    "  readingsCount <- nrow(daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ])\n",
    "  if(readingsCount <= 50){\n",
    "    # if the number of values is not sufficient we remove the null rows for the given client\n",
    "    daily_dataset.filtered <- daily_dataset.filtered[!(daily_dataset.filtered$LCLid == lcl & is.na(daily_dataset.filtered$consumption)), ]\n",
    "  }else{\n",
    "    # otherwise we apply the imputation\n",
    "    daily_dataset.filtered.temp <- daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]\n",
    "    daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]$x <- knnImp(daily_dataset.filtered.temp, k = 10)$x\n",
    "  }\n",
    "}\n",
    "daily_dataset.filtered$consumption <- daily_dataset.filtered$x\n",
    "daily_dataset.filtered <- subset(daily_dataset.filtered, select = -x )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afcc369",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# STEP 3: EXPLORATORY ANALYSIS\n",
    "## 3.1: analysis of the seasonal components\n",
    "\n",
    "In this step, we embark on an exploratory analysis to discover trends and seasonality within the consumption patterns of the merged daily dataset. For better clarity and ease of analysis, a seasonal decomposition is strategically applied to two years of data. This choice allows for a more granular examination of the underlying patterns, providing a deeper understanding of both weekly and yearly seasonal components.\n",
    "\n",
    "The code snippet initiates the process by aggregating daily consumption data, followed by the application of a seasonal-trend decomposition using LOESS (STL) method. Notably, to extract additional trend components, we specifically feed only the trend component of the weekly STL decomposition into subsequent analyses. This deliberate choice allows for a better separation between the analysis of the weekly and yearly seasonal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fe867",
   "metadata": {
    "fig.show": "hold",
    "lines_to_next_cell": 0,
    "message": false,
    "out.width": "50%",
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Aggregate daily consumption data to obtain a mean value for each day\n",
    "daily_dataset.consumption <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))\n",
    "daily_dataset.consumption <- daily_dataset.consumption[daily_dataset.consumption$day >= \"2012-01-01\" & daily_dataset.consumption$day <= \"2014-01-10\",]\n",
    "\n",
    "# analysis of trend and seasonality\n",
    "ts_consumption.week <- ts(daily_dataset.consumption$x, frequency = 7)\n",
    "ts_consumption.week.decom <- stl(ts_consumption.week,s.window=\"periodic\")\n",
    "\n",
    "plot( ts_consumption.week.decom , main=\"WEEKLY SEASONAL COMPONENTS DECOMPOSITION\")\n",
    "\n",
    "ts_consumption.year <- ts(na.omit(ts_consumption.week.decom$time.series[, \"trend\"]), frequency = 364)\n",
    "ts_consumption.year.decom <- stl(ts_consumption.year,s.window=\"periodic\")\n",
    "\n",
    "plot( ts_consumption.year.decom , main=\"YEARLY SEASONAL COMPONENTS DECOMPOSITION\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23fef5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The first graph showcases the decomposition of weekly seasonal components. Each peak and trough in the plot corresponds to a specific day of the week, reflecting the recurring patterns within a seven-day cycle. For instance, noticeable fluctuations might be observed, indicating higher consumption on certain days, potentially influenced by factors such as weekdays or weekends.\n",
    "\n",
    "The second graph depicts the decomposition of yearly seasonal components. In this visualization, the focus shifts to identifying patterns that repeat annually. Peaks and valleys within the plot represent the seasonal variations in energy consumption that occur over the course of a year. This could be influenced by various external factors such as weather changes, holidays, or seasonal trends in consumer behavior. \n",
    "\n",
    "In addition to the discernible weekly and yearly components identified in the decomposition graphs, it's crucial to acknowledge the significant variance attributable to the inherent randomness associated with weather conditions. This factor becomes particularly pronounced, given that the dataset has been recorded in London. The unpredictable nature of weather patterns, including temperature fluctuations, precipitation, and sunlight duration, introduces a level of randomness that contributes to the overall variability in energy consumption. \n",
    "\n",
    "Continuing our exploration of the dataset, the subsequent code snippet delves into an analysis of the random components within the time series. By extracting only the stochastic elements, we aim to unveil patterns that contribute to the variance in energy consumption. Notably, this investigation reveals a dynamic aspect of the dataset, demonstrating that the variance is not uniform across time but rather exhibits some degree of fluctuations. Specifically, there is an intensification of variance during a particular period of the year, notably during the winter season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c8ff3",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ts_consumption.week.decom.remainder <- ts(na.omit(ts_consumption.week.decom$time.series[, \"remainder\"]), frequency = 364)\n",
    "ts_consumption.year.decom.remainder <- ts(na.omit(ts_consumption.year.decom$time.series[, \"remainder\"]), frequency = 364)\n",
    "\n",
    "plot( ts_consumption.week.decom.remainder + ts_consumption.year.decom.remainder, xlab=\"Time (in YEARS)\", ylab=\"consumption variance\", main=\"WEEKLY VARIANCE + YEARLY VARIANCE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661b088",
   "metadata": {},
   "source": [
    "Following the decomposition of both weekly and yearly components, the focus shifts to verifying whether any additional meaningful seasonality can be discerned in the residual variance. Spectral analysis serves as a powerful tool in this exploration, allowing us to scrutinize the frequency domain of the data and identify potential periodic patterns that may not have been captured by previous decomposition methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2d05e",
   "metadata": {
    "fig.show": "hold",
    "lines_to_next_cell": 0,
    "message": false,
    "out.width": "50%",
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "ts_consumption <- ts(daily_dataset.consumption$x)\n",
    "ts_variance_consumption <- ts(ts_consumption.week.decom.remainder + ts_consumption.year.decom.remainder)\n",
    "\n",
    "# Perform spectral analysis and extract frequency of seasonality\n",
    "spectrum(ts_consumption, span=5, log = c(\"no\"), main=\"seasonality before STL\")\n",
    "spec_result <- spectrum(ts_variance_consumption, span=5, log = c(\"no\"), ylim=c(0, 248.5), main=\"seasonality after STL\")\n",
    "\n",
    "# Extract the frequency corresponding to the maximum power\n",
    "freq <- spec_result$freq\n",
    "power <- spec_result$spec\n",
    "max_power_index <- which.max(power)\n",
    "max_frequency.period <- 1/freq[max_power_index]\n",
    "cat(\"periodicity (in days) of the most notable seasonality remaining after STL decomposition: \", max_frequency.period)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e4127",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Analyzing the graphs above, it becomes evident that the most crucial seasonal components have been successfully extracted from the dataset. The remaining seasonality exhibits a \"spectrum\" value representing a fraction of the initially observed seasonality, with a periodicity equal to 75 (which doesn't represent any significat yearly occurring seasonal component).\n",
    "\n",
    "## 3.2: analysis of the effects of the weather releted heterogeneous variables\n",
    "To further unravel the impact of weather conditions on the time series, the next step involves eliminating the weekly seasonality and yearly trend from the dataset. This selective removal aims to isolate the components previously identified as variance and those most likely influenced by weather conditions ([ASSUMPTION n°4](#assumptions))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1856110",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# plot only weather influenced components\n",
    "ts_consumption.year.decom.seasonal <- ts(na.omit(ts_consumption.year.decom$time.series[, \"seasonal\"]), frequency = 364)\n",
    "plot( ts_consumption.week.decom.remainder + ts_consumption.week.decom.remainder + ts_consumption.year.decom.seasonal, xlab=\"Time (in YEARS)\", ylab=\"consumption variance\", main=\"WEEKLY VARIANCE + YEARLY VARIANCE + YEARLY SEASONALITY\")\n",
    "\n",
    "# remove non weather influenced components from daily_dataset.consumption and merge with weather_daily_darksky\n",
    "daily_dataset.consumption$x <- daily_dataset.consumption$x - ts(na.omit(ts_consumption.week.decom$time.series[, \"seasonal\"]), frequency = 364) - ts(na.omit(ts_consumption.year.decom$time.series[, \"trend\"]), frequency = 364)\n",
    "\n",
    "weather_daily_darksky <- subset(weather_daily_darksky, select = -x )\n",
    "\n",
    "daily_dataset.consumption <- daily_dataset.consumption %>%\n",
    "  inner_join(weather_daily_darksky, by = c(\"day\" = \"day\")) %>%\n",
    "  dplyr::select(\n",
    "    day,\n",
    "    x,\n",
    "    weather_condition,\n",
    "    cloudCover,\n",
    "    windBearing,\n",
    "    windSpeed,\n",
    "    pressure,\n",
    "    visibility,\n",
    "    humidity,\n",
    "    precipType,\n",
    "    uvIndex,\n",
    "    sunlightHours_numeric,\n",
    "    temperatureMin,\n",
    "    temperatureMax\n",
    "  )\n",
    "\n",
    "daily_dataset.consumption <- daily_dataset.consumption %>%\n",
    "  mutate(\n",
    "    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),\n",
    "    dayOfWeek = wday(day, week_start=1),\n",
    "    dayOfYear = as.numeric(strftime(day, format = \"%j\")),\n",
    "    consumption = x\n",
    "  )\n",
    "\n",
    "daily_dataset.consumption <- subset(daily_dataset.consumption, select = -x )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364d3b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    " To facilitate this analysis, certain character variables undergo a transformation into numeric values. This conversion involves replacing the original values with their respective indices within a vector that encompasses all values, sorted by mean consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d38aab",
   "metadata": {
    "message": false,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Sort weather conditions based on mean consumption\n",
    "weather_condition_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(weather_condition=daily_dataset.filtered$weather_condition), FUN=mean)\n",
    "weather_condition_sorted <- weather_condition_sorted[order(weather_condition_sorted$x), ]$weather_condition\n",
    "# Assign numeric indices to weather conditions in the main dataset\n",
    "daily_dataset.consumption$weather_condition <- match(daily_dataset.consumption$weather_condition, weather_condition_sorted)\n",
    "\n",
    "# Sort precipitation types based on mean consumption\n",
    "precipType_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(precipType=daily_dataset.filtered$precipType), FUN=mean)\n",
    "precipType_sorted <- precipType_sorted[order(precipType_sorted$x), ]$precipType\n",
    "# Assign numeric indices to precipitation types in the main dataset\n",
    "daily_dataset.consumption$precipType <- match(daily_dataset.consumption$precipType, precipType_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c06f2-4148-4f21-9514-a2fd9330c8e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In the following code snippet, with all heterogeneous variables now transformed into numeric representations, we analyze the correlation patterns within the dataset. The correlation matrix and plot will be generated to clarify the relationships between different variables and their impact on energy consumption. Specifically, the focus will be on the Pearson correlation coefficient, as it emerged as the metric with the highest degree of correlation among all variables. While Spearman and Kendall coefficients were also considered, the Pearson correlation stands out for its stronger correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84052e",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "corr_matrix.pearson <- rcorr(as.matrix(daily_dataset.consumption[, c(-1, -14)]), type = \"pearson\")\n",
    "corrplot(corr_matrix.pearson[[1]][1:14, 15, drop=FALSE], cl.pos='n', method = \"number\", mar=c(0,0,1,0) ,title = \"Pearson correlation for consumption\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273becd8",
   "metadata": {},
   "source": [
    "The correlation analysis has yielded valuable insights into the impact of various factors on energy consumption, particularly after the removal of the weekly component of seasonality. Notably, the \"dayOfWeek\" endogenous variable demonstrates minimal correlation with the consumption target variable once weekly seasonality is accounted for. Additionally, variables such as \"windBearing\" and \"pressure\" exhibit limited correlation with the target variable. As a result, these variables are deemed to have a restricted effect when evaluating the influence of weather conditions on consumption. In the context of implementing neural networks or forecasting techniques for predicting future values in the time series, the decision to exclude these variables is strategic.\n",
    "\n",
    "# STEP 4: APPLY FORECASTING TECHNIQUES (holt-winters, ARIMA, Regression Analysis)\n",
    "In the upcoming chapter, we delve into the heart of predictive modeling to anticipate future trends in energy consumption. This pivotal step involves the application of diverse forecasting methodologies, including the Holt-Winters method, ARIMA (AutoRegressive Integrated Moving Average), and Regression Analysis. Each technique offers unique insights and capabilities, contributing to a comprehensive and accurate forecasting framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe49936",
   "metadata": {
    "fig.show": "hold",
    "message": false,
    "out.width": "50%",
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "# prepare aggregated dataset\n",
    "daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))\n",
    "daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= \"2012-01-01\" & daily_dataset.tmp$day <= \"2014-02-27\",]\n",
    "ts_consumption.month <- ts(daily_dataset.tmp$x, frequency = 360)\n",
    "\n",
    "# Apply Holt-Winters method with additive seasonality\n",
    "ts_consumption.month.HW <- HoltWinters(ts_consumption.month, seasonal = \"additive\")\n",
    "\n",
    "#forecast one year into the future\n",
    "ts_consumption.month.HW.predict <- predict(ts_consumption.month.HW, 360, prediction.interval = TRUE, level=0.5)\n",
    "\n",
    "x<-plot(ts_consumption.month, ylab=\"consumption\", xlim=c(1, 4.2), ylim=c(4, 16.5), main=\"holt-winters forecast\") + lines(ts_consumption.month.HW$fitted[,1], lty=2, col=\"blue\") + lines(ts_consumption.month.HW.predict[,1], col=\"orange\") + lines(ts_consumption.month.HW.predict[,2], col=\"green\") + lines(ts_consumption.month.HW.predict[,3], col=\"red\")\n",
    "\n",
    "# apply ARIMA forecasting technique\n",
    "ts_consumption.month.ARIMA <- auto.arima(ts_consumption.month)\n",
    "\n",
    "#forecast one year into the future\n",
    "ts_consumption.month.ARIMA.predict <- forecast(ts_consumption.month.ARIMA, h = 360, level = 90)\n",
    "\n",
    "plot(ts_consumption.month.ARIMA.predict, ylab = \"consumption\", xlim = c(1, 4.2), ylim = c(4.5, 16.5), main=\"ARIMA forecast\") \n",
    "lines(window(fitted(ts_consumption.month.ARIMA, start = 2), start = 2), col = \"blue\", lty = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedfef1",
   "metadata": {},
   "source": [
    "It's imperative to note a crucial distinction in the confidence levels (CL) between the Holt-Winters and ARIMA forecasting models. The Holt-Winters model, owing to the high variance in the time series data, produces the above graph only when employing a confidence level of 50% (0.5). This lower confidence level signifies a wider range of uncertainty in the predictions, highlighting the inherent challenge posed by the considerable variability in the dataset.\n",
    "\n",
    "Conversely, the ARIMA model emerges with results characterized by a significantly higher confidence level of 90%. This notable increase in confidence levels reflects a more robust and precise prediction capability. The ARIMA model's ability to provide forecasts with greater confidence underscores its effectiveness in capturing the underlying patterns and trends within the time series data.\n",
    "\n",
    "\n",
    "In the upcoming code snippet, we implemented the Regression Analysis forecasting technique. Unlike time series-specific methods, regression analysis leverages a multifaceted approach by considering various predictor variables. The dataset is prepared, incorporating not only temporal features but also meteorological and calendar-related factors, such as weather conditions, holidays, and day-specific attributes.\n",
    "\n",
    "The regression model is then trained on this dataset to predict energy consumption. The resulting plot compares the observed consumption values with the predictions generated by the regression modes.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d500343",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "#create temporary dataset\n",
    "daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))\n",
    "daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= \"2012-01-01\" & daily_dataset.tmp$day <= \"2014-02-27\",]\n",
    "daily_dataset.tmp <- daily_dataset.tmp %>%\n",
    "  mutate(\n",
    "    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),\n",
    "    dayOfWeek = wday(day, week_start=1),\n",
    "    dayOfYear = as.numeric(strftime(day, format = \"%j\")),\n",
    "    year = as.numeric(strftime(day, format = \"%Y\")),\n",
    "    consumption = x\n",
    "  ) \n",
    "daily_dataset.tmp <- daily_dataset.tmp %>%\n",
    "  inner_join(weather_daily_darksky, by = c(\"day\" = \"day\")) %>%\n",
    "  dplyr::select(\n",
    "    day,\n",
    "    x,\n",
    "    dayOfWeek,\n",
    "    dayOfYear,\n",
    "    year,\n",
    "    weather_condition,\n",
    "    cloudCover,\n",
    "    windSpeed,\n",
    "    visibility,\n",
    "    humidity,\n",
    "    precipType,\n",
    "    uvIndex,\n",
    "    temperatureMin,\n",
    "    temperatureMax,\n",
    "    sunlightHours_numeric\n",
    "  )\n",
    "daily_dataset.tmp$weather_condition <- match(daily_dataset.tmp$weather_condition, weather_condition_sorted)\n",
    "daily_dataset.tmp$precipType <- match(daily_dataset.tmp$precipType, precipType_sorted)\n",
    "\n",
    "# Prepare the data for regression\n",
    "model_data <- daily_dataset.tmp %>%\n",
    "  dplyr::select(-day)  # Exclude the 'day' column for regression\n",
    "\n",
    "# Fit the regression model\n",
    "reg_analisys.model <- lm(x ~ ., data = model_data)\n",
    "\n",
    "plot_data <- data.frame(day = daily_dataset.tmp$day, observed = daily_dataset.tmp$x, predicted = predict(reg_analisys.model, daily_dataset.tmp))\n",
    "ggplot(plot_data, aes(x = day)) +\n",
    "  geom_line(aes(y = observed, color = \"Observed\"), linewidth = 1) +\n",
    "  geom_line(aes(y = predicted, color = \"Predicted\"), linewidth = 1, linetype = \"dashed\") +\n",
    "  labs(title = \"regression analisys model\",\n",
    "       x = \"Day\",\n",
    "       y = \"Consumption\") +\n",
    "  theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aae072",
   "metadata": {},
   "source": [
    "# STEP 5: ASSESS FORECASTING TECHNIQUES\n",
    "In the upcoming section, we undertake an assessment of the various forecasting techniques employed thus far. For each model, a comprehensive assessment will be conducted, considering key metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), bias, Percentage of Correctly Predicted Directions (PCPD), and R-squared (R²). These metrics collectively provide further understanding of the models' performance, highlighting their strengths and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2fa02",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Define function to calculate PCPD\n",
    "pcpd <- function(actual, forecast) {\n",
    "  direction_actual <- diff(actual) > 0\n",
    "  direction_forecast <- diff(forecast) > 0\n",
    "  correct_direction <- sum(direction_actual == direction_forecast)\n",
    "  total_directions <- length(direction_actual) - 1  # Exclude the first data point\n",
    "  return(correct_direction / total_directions)\n",
    "}\n",
    "\n",
    "# Define function to calculate R2\n",
    "RSQUARE = function(y_actual,y_predict){\n",
    "  cor(y_actual,y_predict)^2\n",
    "}\n",
    "\n",
    "# Calculate metrics for Holt-Winters model\n",
    "mae_hw <- mean(abs(ts_consumption.month - ts_consumption.month.HW$fitted[,1]))\n",
    "mse_hw <- mean((ts_consumption.month - ts_consumption.month.HW$fitted[,1])^2)\n",
    "bias_hw <- mean(ts_consumption.month - ts_consumption.month.HW$fitted[,1])\n",
    "pcpd_hw <- pcpd(window(ts_consumption.month, start = 2), ts_consumption.month.HW$fitted[,1])\n",
    "r_squared_hw <- RSQUARE(window(ts_consumption.month, start = 2), ts_consumption.month.HW$fitted[,1])\n",
    "\n",
    "# Calculate metrics for ARIMA model\n",
    "mae_arima <- mean(abs(ts_consumption.month - fitted(ts_consumption.month.ARIMA)))\n",
    "mse_arima <- mean((ts_consumption.month - fitted(ts_consumption.month.ARIMA))^2)\n",
    "bias_arima <- mean(ts_consumption.month - fitted(ts_consumption.month.ARIMA))\n",
    "pcpd_arima <- pcpd(ts_consumption.month, fitted(ts_consumption.month.ARIMA))\n",
    "r_squared_arima <- RSQUARE(ts_consumption.month, fitted(ts_consumption.month.ARIMA))\n",
    "\n",
    "# Calculate metrics for Regression Analysis model\n",
    "mae_reg <- mean(abs(plot_data$observed - plot_data$predicted))\n",
    "mse_reg <- mean((plot_data$observed - plot_data$predicted)^2)\n",
    "bias_reg <- mean(plot_data$observed - plot_data$predicted)\n",
    "pcpd_reg <- pcpd(plot_data$observed, plot_data$predicted)\n",
    "r_squared_reg <- RSQUARE(plot_data$observed, plot_data$predicted)\n",
    "\n",
    "# Create a data frame with the results and display them\n",
    "results_df <- data.frame(\n",
    "  Model = c(\"Holt-Winters\", \"ARIMA\", \"Regression Analysis\"),\n",
    "  MAE = c(mae_hw, mae_arima, mae_reg),\n",
    "  MSE = c(mse_hw, mse_arima, mse_reg),\n",
    "  Forecast_Bias = c(bias_hw, bias_arima, bias_reg),\n",
    "  PCPD = c(pcpd_hw, pcpd_arima, pcpd_reg),\n",
    "  R_squared = c(r_squared_hw, r_squared_arima, r_squared_reg)\n",
    ")\n",
    "\n",
    "kable(results_df, caption = \"Model Comparison Results\", format = \"markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ddc6e1",
   "metadata": {},
   "source": [
    "Given the distinct objectives of the three models, Holt-Winters (HW), ARIMA, and Regression Analysis (RA), are tailored for different forecasting horizons, their comparative performance reveals diverse strengths and weaknesses. Holt-Winters and ARIMA shows effectiveness in the medium to long-term predictions, spanning from a few months to a year or maby more, while Regression Analysis excels in short-term predictions, leveraging weather conditions (to be known beforehand) for forecasts.\n",
    "\n",
    "Evaluating the metrics across these models, ARIMA emerges as the contender with the lowest overall error, boasting a Mean Absolute Error (MAE) of 0.241, a Percentage of Correct Prediction Direction (PCPD) at 75.644%, and the highest explained variance (R-squared) of 94.740%. However, it is noteworthy that ARIMA tends to produce more \"large\" errors compared to the Holt-Winters model, evident in its larger Mean Squared Error (MSE). On the other hand, both ARIMA and Holt-Winters exhibit some degree of bias, albeit limited, toward the conservative side in their predictions. In contrast, the Regression Analysis model consistently yields errors whose mean is closely aligned with actual consumption.\n",
    "\n",
    "Overall, whenever the objective is to forecast the consumption of the distribution network as a whole or to predict the behaviour of a large population of consumers, the best use for these models would be to:\n",
    "1. adopt the Regression Analysis model to make predictions of up to one to two week into the future while still comparing its results with those obtained from the Arima model\n",
    "2. Use the Arima model for any other type of prediction\n",
    "\n",
    "# STEP 6: MULTIVARIATE REGRESSION OF MEAN AND MAXIMUM DAILY USER CONSUMPTIONS\n",
    "In the final phase, Step 6, our focus shifts to the development of a comprehensive model capable of predicting both mean and maximum daily user consumptions that could easily find application given the context. This holistic approach is designed to account for the maximum load that certain users might exhibit, which could then be accounted for when sizing the network infrastructure. \n",
    "\n",
    "Recognizing the potential impact of outliers, particularly in determining maximum values, we employ two distinct techniques for mitigation:\n",
    "1. the calculation of the maximum value as the mean between the three highest consumers of the day.\n",
    "2. the implementation of a moving average on three lags to further dampen the influence of outliers on the maximum consumption value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90102764",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "#aggregate to obtain the max and mean daily consumption\n",
    "daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=function(x) {\n",
    "  sorted_values <- sort(x, decreasing=TRUE)\n",
    "  top_three_mean <- mean(sorted_values[1:3])\n",
    "  c(max=top_three_mean, mean=mean(x))\n",
    "}))\n",
    "daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= \"2012-06-01\" & daily_dataset.tmp$day <= \"2014-02-27\",]\n",
    "daily_dataset.tmp <- daily_dataset.tmp %>%\n",
    "  mutate(\n",
    "    x.mean = x[, \"mean\"],\n",
    "    x.max = x[, \"max\"]\n",
    "  ) \n",
    "\n",
    "# implementation of the 3 lag centered moving average\n",
    "daily_dataset.tmp$x.max <- stats::filter(daily_dataset.tmp$x.max, rep(1/3, 3), sides = 2)\n",
    "daily_dataset.tmp$x.mean <- stats::filter(daily_dataset.tmp$x.mean, rep(1/3, 3), sides = 2)\n",
    "\n",
    "#plot the resulting dataset\n",
    "plot(daily_dataset.tmp$day, daily_dataset.tmp$x.max, type=\"l\", col=\"red\", lty=1, ylim=c(0, 220), main=\"Daily Consumption Statistics\", xlab=\"Day\", ylab=\"Consumption\")\n",
    "lines(daily_dataset.tmp$day, daily_dataset.tmp$x.mean, col=\"blue\", lty=1)\n",
    "legend(\"topright\", legend=c(\"Max\", \"Mean\"), col=c(\"red\", \"blue\"), lty=1:1)\n",
    "\n",
    "# Prepare the data for multivariate regression\n",
    "ts_data <- na.omit(daily_dataset.tmp) %>% dplyr::select(-day) %>% dplyr::select(-x)  # Exclude the 'day' column for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20821d37",
   "metadata": {},
   "source": [
    "To accomplish the task of predicting both mean and maximum daily user consumptions, we employ a Vector Auto Regression (VAR) model. The VAR model is particularly well-suited for multivariate time series analysis, allowing us to simultaneously capture the dynamic interactions and dependencies among multiple variables. In this context, the VAR model facilitates the prediction of both mean and maximum consumption values by considering their interrelated behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eefbd6",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "ts_data <- ts(ts_data, start = c(1, 1), frequency = 1)\n",
    "\n",
    "# Using VARselect to choose the optimal p value\n",
    "var_select <- VARselect(ts_data, lag.max = 120, type = \"both\")\n",
    "#print(var_select)\n",
    "\n",
    "var_model <- VAR(ts_data, p = 32)\n",
    "var_forecast <- forecast(var_model, h = 30, level = 50)\n",
    "\n",
    "plot(var_forecast, main=\"Forecast for mean and max consumption\", xlim=c(200, 650))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35300c74",
   "metadata": {},
   "source": [
    "In line with the previously fitted models, the forecast, at a 50% confidence level, generates a graph with minimum and maximum plausible values that deviate significantly from the actual forecast. Nevertheless, this model exhibits an acceptable level of accuracy, particularly when making predictions in the medium to short term, roughly around one month. It is plausible that the model's performance could be further enhanced by progressively expanding the dimension of the training dataset over time.\n",
    "\n",
    "# CONCLUSION\n",
    "Our comprehensive analysis of electricity consumption patterns has provided valuable insights and predictive models. We began by establishing key assumptions and meticulously organizing the dataset. Exploring, cleaning, and transforming the data revealed critical patterns and necessitated strategic imputation. Through exploratory and variance analyses, we unveiled seasonal components and identified periods of heightened variability. Correlation analyses guided feature selection, while forecasting techniques like Holt-Winters, ARIMA, and Regression Analysis were applied and evaluated. The adoption of the Vector Auto Regression model further enhanced our forecasting capabilities, considering both mean and maximum daily user consumptions.\n",
    "\n",
    "In conclusion, this comprehensive analysis not only deepened our understanding of electricity consumption dynamics but also showcased the effectiveness of diverse forecasting techniques. The iterative nature of the process, coupled with continuous refinement, positions us to make informed decisions, optimize energy management, and adapt to evolving consumption patterns in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc4f11",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "vscode": {
     "languageId": "r"
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "#extracting CSV dataFrame for NN training in Python\n",
    "daily_dataset.tmp <- daily_dataset.filtered\n",
    "daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= \"2012-01-01\" & daily_dataset.tmp$day <= \"2014-02-27\",]\n",
    "daily_dataset.tmp <- daily_dataset.tmp %>%\n",
    "  mutate(\n",
    "    year = as.numeric(strftime(day, format = \"%Y\")),\n",
    "  )\n",
    "\n",
    "stdorToU_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(stdorToU=daily_dataset.filtered$stdorToU), FUN=mean)\n",
    "stdorToU_sorted <- stdorToU_sorted[order(stdorToU_sorted$x), ]$stdorToU\n",
    "\n",
    "daily_dataset.tmp$weather_condition <- match(daily_dataset.tmp$weather_condition, weather_condition_sorted)\n",
    "daily_dataset.tmp$precipType <- match(daily_dataset.tmp$precipType, precipType_sorted)\n",
    "daily_dataset.tmp$stdorToU <- match(daily_dataset.tmp$stdorToU, stdorToU_sorted)\n",
    "\n",
    "write.csv(daily_dataset.tmp, file = \".\\\\archive\\\\final_dataset.csv\", row.names = FALSE)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "fig.cap,warning,message,tags,fig.show,out.width,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
