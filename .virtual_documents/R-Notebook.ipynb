


# packages to install
# install.packages("performanceEstimation")
# install.packages("fpp3")
# install.packages("knitr")
# install.packages("vars")

library(dplyr)
library(lubridate)
library(ggplot2)
library(ggplot2)
library(performanceEstimation)
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)
library(forecast)
library(knitr)
library(vars)





daily_dataset <- ".\\archive\\daily_dataset.csv"
informations_households <- ".\\archive\\informations_households.csv"
uk_bank_holidays <- ".\\archive\\uk_bank_holidays.csv"
weather_daily_darksky <- ".\\archive\\weather_daily_darksky.csv"

# Import the CSV files
daily_dataset <- read.csv(daily_dataset)
informations_households <- read.csv(informations_households)
uk_bank_holidays <- read.csv(uk_bank_holidays)
weather_daily_darksky <- read.csv(weather_daily_darksky)

# Augment daily_dataset: flag UK holiday days, add DOW and DOY columns,
daily_dataset.augmented <- daily_dataset %>%
  mutate(
      isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
      day = as.Date(day, format = "%Y-%m-%d"),
      dayOfWeek = wday(day, week_start=1),
      dayOfYear = as.numeric(strftime(day, format = "%j")),
      consumption = energy_sum
  )

# Augment weather_daily_darksky: add sunlight hours column, correct format of day column and remove duplicates
weather_daily_darksky <- weather_daily_darksky %>%
    mutate(
        sunlightHours = difftime(sunsetTime, sunriseTime, units = "hours"),
        weather_condition = icon
    )
weather_daily_darksky$day <- as.Date(weather_daily_darksky$time)
weather_daily_darksky <- weather_daily_darksky[!duplicated(weather_daily_darksky$day), ]

# Merge datasets and select only the useful columns
daily_dataset.merged <- daily_dataset.augmented %>%
    inner_join(informations_households, by = "LCLid") %>%
    inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
    dplyr::select(
        LCLid,
        day,
        consumption,
        isUkHoliday,
        dayOfWeek,
        dayOfYear,
        stdorToU,
        weather_condition,
        cloudCover,
        windBearing,
        windSpeed,
        pressure,
        visibility,
        humidity,
        precipType,
        uvIndex,
        sunlightHours,
        temperatureMin,
        temperatureMax
    )





freq_table <- table(daily_dataset$day)
freq_data <- as.data.frame(table(daily_dataset$day))
freq_data$day <- as.Date(names(freq_table))

threshold <- 0.75 * max(freq_data$Freq)

ggplot(freq_data, aes(x = day, y = Freq)) +
  geom_line() +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red") +
  labs(title = "Daily Dataset Frequency Plot", x = "Day", y = "Frequency") +
  theme_minimal()


# filtering values whose frequency is lower than threshold
daily_dataset.filtered <- daily_dataset.merged[daily_dataset.merged$day %in% freq_data$day[freq_data$Freq >= threshold], ]

unfiltered_data_points.mln <- round(nrow(daily_dataset.merged) / 1000000, 1)
filtered_data_points.mln <- round(nrow(daily_dataset.filtered) / 1000000, 1)

cat("Unfiltered Data Points (in millions):", unfiltered_data_points.mln, "\n")
cat("Filtered Data Points (in millions):", filtered_data_points.mln, "\n")





# identifying missing values

summary(daily_dataset.filtered)

consumption_missing_values <- sum(is.na(daily_dataset.filtered$consumption))
cloudCover_missing_values <- sum(is.na(daily_dataset.filtered$cloudCover))
uvIndex_missing_values <- sum(is.na(daily_dataset.filtered$uvIndex))

cat("Consumption Missing Values:", consumption_missing_values, "\n")
cat("Cloud Cover Missing Values:", cloudCover_missing_values, "\n")
cat("UV Index Missing Values:", uvIndex_missing_values, "\n")

cc_uv_missing_values.date <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$cloudCover) & is.na(daily_dataset.filtered$uvIndex), ]$day) # nolint: line_length_linter.
cat("Unique Dates with Both Cloud Cover and UV Index Missing Values:", format(cc_uv_missing_values.date, format = "%d %B %Y"), "\n")

consumption_missing_values.dates.num <- length(unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$day)) # nolint: line_length_linter.
cat("Number of Unique Dates with Consumption Missing Values:", consumption_missing_values.dates.num, "\n")

values_cc_uv_missing_values.dates <- sum(daily_dataset.filtered$day == as.Date(cc_uv_missing_values.date))
cat("Number of values for the [", format(cc_uv_missing_values.date, format = "%d %B %Y"), "]: ", values_cc_uv_missing_values.dates, "\n")





# Cloud Cover and UV Index Imputation of Missing Values

# Impute missing values for UV Index and Cloud Cover in the 'weather_daily_darksky' table
weather_daily_darksky$sunlightHours_numeric <- as.numeric(weather_daily_darksky$sunlightHours)

# Define features for imputation in 'weather_daily_darksky'
features <- c("temperatureMax", "temperatureMin", "windBearing", "dewPoint", "windSpeed", "pressure", "visibility", "humidity", "sunlightHours_numeric", "x")

# Impute values
weather_daily_darksky$x <- as.numeric(weather_daily_darksky$uvIndex)
weather_daily_darksky$uvIndex <- knnImp(weather_daily_darksky[, features], k = 10)$x
weather_daily_darksky$x <- as.numeric(weather_daily_darksky$cloudCover)
weather_daily_darksky$cloudCover <- knnImp(weather_daily_darksky[, features], k = 10)$x

# Re-Merge and select relevant columns in 'daily_dataset.filtered'
daily_dataset.filtered <- daily_dataset.filtered[, !(names(daily_dataset.filtered) %in% setdiff(colnames(weather_daily_darksky), "day"))]
daily_dataset.filtered <- daily_dataset.filtered %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  dplyr::select(
    LCLid,
    day,
    consumption,
    isUkHoliday,
    dayOfWeek,
    dayOfYear,
    stdorToU,
    weather_condition,
    cloudCover,
    windBearing,
    windSpeed,
    pressure,
    visibility,
    humidity,
    precipType,
    uvIndex,
    sunlightHours,
    temperatureMin,
    temperatureMax,
    sunlightHours_numeric
  )

daily_dataset.filtered$sunlightHours <- as.numeric(daily_dataset.filtered$sunlightHours)

# Define the features for imputation
features <- c("dayOfYear", "dayOfWeek", "temperatureMax", "temperatureMin", "windBearing", "windSpeed", "pressure", "visibility", "humidity", "sunlightHours", "x", "uvIndex", "cloudCover")

# imputate on a per-user basis
LCLid.list <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$LCLid)

# Iterate through users for consumption imputation
daily_dataset.filtered$x <- as.numeric(daily_dataset.filtered$consumption)
for(lcl in LCLid.list) {
  lcl_rows = daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ]
  readingsCount <- nrow(daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ])
  if(readingsCount <= 50){
    # if the number of values is not sufficient we remove the null rows for the given client
    daily_dataset.filtered <- daily_dataset.filtered[!(daily_dataset.filtered$LCLid == lcl & is.na(daily_dataset.filtered$consumption)), ]
  }else{
    # otherwise we apply the imputation
    daily_dataset.filtered.temp <- daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]
    daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]$x <- knnImp(daily_dataset.filtered.temp, k = 10)$x
  }
}
daily_dataset.filtered$consumption <- daily_dataset.filtered$x
daily_dataset.filtered <- subset(daily_dataset.filtered, select = -x )






# Aggregate daily consumption data to obtain a mean value for each day
daily_dataset.consumption <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))
daily_dataset.consumption <- daily_dataset.consumption[daily_dataset.consumption$day >= "2012-01-01" & daily_dataset.consumption$day <= "2014-01-10",]

# analysis of trend and seasonality
ts_consumption.week <- ts(daily_dataset.consumption$x, frequency = 7)
ts_consumption.week.decom <- stl(ts_consumption.week,s.window="periodic")

plot( ts_consumption.week.decom , main="WEEKLY SEASONAL COMPONENTS DECOMPOSITION")

ts_consumption.year <- ts(na.omit(ts_consumption.week.decom$time.series[, "trend"]), frequency = 364)
ts_consumption.year.decom <- stl(ts_consumption.year,s.window="periodic")

plot( ts_consumption.year.decom , main="YEARLY SEASONAL COMPONENTS DECOMPOSITION" )







ts_consumption.week.decom.remainder <- ts(na.omit(ts_consumption.week.decom$time.series[, "remainder"]), frequency = 364)
ts_consumption.year.decom.remainder <- ts(na.omit(ts_consumption.year.decom$time.series[, "remainder"]), frequency = 364)

plot( ts_consumption.week.decom.remainder + ts_consumption.year.decom.remainder, xlab="Time (in YEARS)", ylab="consumption variance", main="WEEKLY VARIANCE + YEARLY VARIANCE")






ts_consumption <- ts(daily_dataset.consumption$x)
ts_variance_consumption <- ts(ts_consumption.week.decom.remainder + ts_consumption.year.decom.remainder)

# Perform spectral analysis and extract frequency of seasonality
spectrum(ts_consumption, span=5, log = c("no"), main="seasonality before STL")
spec_result <- spectrum(ts_variance_consumption, span=5, log = c("no"), ylim=c(0, 248.5), main="seasonality after STL")

# Extract the frequency corresponding to the maximum power
freq <- spec_result$freq
power <- spec_result$spec
max_power_index <- which.max(power)
max_frequency.period <- 1/freq[max_power_index]
cat("periodicity (in days) of the most notable seasonality remaining after STL decomposition: ", max_frequency.period)






# plot only weather influenced components
ts_consumption.year.decom.seasonal <- ts(na.omit(ts_consumption.year.decom$time.series[, "seasonal"]), frequency = 364)
plot( ts_consumption.week.decom.remainder + ts_consumption.week.decom.remainder + ts_consumption.year.decom.seasonal, xlab="Time (in YEARS)", ylab="consumption variance", main="WEEKLY VARIANCE + YEARLY VARIANCE + YEARLY SEASONALITY")

# remove non weather influenced components from daily_dataset.consumption and merge with weather_daily_darksky
daily_dataset.consumption$x <- daily_dataset.consumption$x - ts(na.omit(ts_consumption.week.decom$time.series[, "seasonal"]), frequency = 364) - ts(na.omit(ts_consumption.year.decom$time.series[, "trend"]), frequency = 364)

weather_daily_darksky <- subset(weather_daily_darksky, select = -x )

daily_dataset.consumption <- daily_dataset.consumption %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  dplyr::select(
    day,
    x,
    weather_condition,
    cloudCover,
    windBearing,
    windSpeed,
    pressure,
    visibility,
    humidity,
    precipType,
    uvIndex,
    sunlightHours_numeric,
    temperatureMin,
    temperatureMax
  )

daily_dataset.consumption <- daily_dataset.consumption %>%
  mutate(
    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
    dayOfWeek = wday(day, week_start=1),
    dayOfYear = as.numeric(strftime(day, format = "%j")),
    consumption = x
  )

daily_dataset.consumption <- subset(daily_dataset.consumption, select = -x )





# Sort weather conditions based on mean consumption
weather_condition_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(weather_condition=daily_dataset.filtered$weather_condition), FUN=mean)
weather_condition_sorted <- weather_condition_sorted[order(weather_condition_sorted$x), ]$weather_condition
# Assign numeric indices to weather conditions in the main dataset
daily_dataset.consumption$weather_condition <- match(daily_dataset.consumption$weather_condition, weather_condition_sorted)

# Sort precipitation types based on mean consumption
precipType_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(precipType=daily_dataset.filtered$precipType), FUN=mean)
precipType_sorted <- precipType_sorted[order(precipType_sorted$x), ]$precipType
# Assign numeric indices to precipitation types in the main dataset
daily_dataset.consumption$precipType <- match(daily_dataset.consumption$precipType, precipType_sorted)





corr_matrix.pearson <- rcorr(as.matrix(daily_dataset.consumption[, c(-1, -14)]), type = "pearson")
corrplot(corr_matrix.pearson[[1]][1:14, 15, drop=FALSE], cl.pos='n', method = "number", mar=c(0,0,1,0) ,title = "Pearson correlation for consumption")






# prepare aggregated dataset
daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))
daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= "2012-01-01" & daily_dataset.tmp$day <= "2014-02-27",]
ts_consumption.month <- ts(daily_dataset.tmp$x, frequency = 360)

# Apply Holt-Winters method with additive seasonality
ts_consumption.month.HW <- HoltWinters(ts_consumption.month, seasonal = "additive")

#forecast one year into the future
ts_consumption.month.HW.predict <- predict(ts_consumption.month.HW, 360, prediction.interval = TRUE, level=0.5)

x<-plot(ts_consumption.month, ylab="consumption", xlim=c(1, 4.2), ylim=c(4, 16.5), main="holt-winters forecast") + lines(ts_consumption.month.HW$fitted[,1], lty=2, col="blue") + lines(ts_consumption.month.HW.predict[,1], col="orange") + lines(ts_consumption.month.HW.predict[,2], col="green") + lines(ts_consumption.month.HW.predict[,3], col="red")

# apply ARIMA forecasting technique
ts_consumption.month.ARIMA <- auto.arima(ts_consumption.month)

#forecast one year into the future
ts_consumption.month.ARIMA.predict <- forecast(ts_consumption.month.ARIMA, h = 360, level = 90)

plot(ts_consumption.month.ARIMA.predict, ylab = "consumption", xlim = c(1, 4.2), ylim = c(4.5, 16.5), main="ARIMA forecast") 
lines(window(fitted(ts_consumption.month.ARIMA, start = 2), start = 2), col = "blue", lty = 2)






#create temporary dataset
daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))
daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= "2012-01-01" & daily_dataset.tmp$day <= "2014-02-27",]
daily_dataset.tmp <- daily_dataset.tmp %>%
  mutate(
    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
    dayOfWeek = wday(day, week_start=1),
    dayOfYear = as.numeric(strftime(day, format = "%j")),
    year = as.numeric(strftime(day, format = "%Y")),
    consumption = x
  ) 
daily_dataset.tmp <- daily_dataset.tmp %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  dplyr::select(
    day,
    x,
    dayOfWeek,
    dayOfYear,
    year,
    weather_condition,
    cloudCover,
    windSpeed,
    visibility,
    humidity,
    precipType,
    uvIndex,
    temperatureMin,
    temperatureMax,
    sunlightHours_numeric
  )
daily_dataset.tmp$weather_condition <- match(daily_dataset.tmp$weather_condition, weather_condition_sorted)
daily_dataset.tmp$precipType <- match(daily_dataset.tmp$precipType, precipType_sorted)

# Prepare the data for regression
model_data <- daily_dataset.tmp %>%
  dplyr::select(-day)  # Exclude the 'day' column for regression

# Fit the regression model
reg_analisys.model <- lm(x ~ ., data = model_data)

plot_data <- data.frame(day = daily_dataset.tmp$day, observed = daily_dataset.tmp$x, predicted = predict(reg_analisys.model, daily_dataset.tmp))
ggplot(plot_data, aes(x = day)) +
  geom_line(aes(y = observed, color = "Observed"), linewidth = 1) +
  geom_line(aes(y = predicted, color = "Predicted"), linewidth = 1, linetype = "dashed") +
  labs(title = "regression analisys model",
       x = "Day",
       y = "Consumption") +
  theme_minimal()






# Define function to calculate PCPD
pcpd <- function(actual, forecast) {
  direction_actual <- diff(actual) > 0
  direction_forecast <- diff(forecast) > 0
  correct_direction <- sum(direction_actual == direction_forecast)
  total_directions <- length(direction_actual) - 1  # Exclude the first data point
  return(correct_direction / total_directions)
}

# Define function to calculate R2
RSQUARE = function(y_actual,y_predict){
  cor(y_actual,y_predict)^2
}

# Calculate metrics for Holt-Winters model
mae_hw <- mean(abs(ts_consumption.month - ts_consumption.month.HW$fitted[,1]))
mse_hw <- mean((ts_consumption.month - ts_consumption.month.HW$fitted[,1])^2)
bias_hw <- mean(ts_consumption.month - ts_consumption.month.HW$fitted[,1])
pcpd_hw <- pcpd(window(ts_consumption.month, start = 2), ts_consumption.month.HW$fitted[,1])
r_squared_hw <- RSQUARE(window(ts_consumption.month, start = 2), ts_consumption.month.HW$fitted[,1])

# Calculate metrics for ARIMA model
mae_arima <- mean(abs(ts_consumption.month - fitted(ts_consumption.month.ARIMA)))
mse_arima <- mean((ts_consumption.month - fitted(ts_consumption.month.ARIMA))^2)
bias_arima <- mean(ts_consumption.month - fitted(ts_consumption.month.ARIMA))
pcpd_arima <- pcpd(ts_consumption.month, fitted(ts_consumption.month.ARIMA))
r_squared_arima <- RSQUARE(ts_consumption.month, fitted(ts_consumption.month.ARIMA))

# Calculate metrics for Regression Analysis model
mae_reg <- mean(abs(plot_data$observed - plot_data$predicted))
mse_reg <- mean((plot_data$observed - plot_data$predicted)^2)
bias_reg <- mean(plot_data$observed - plot_data$predicted)
pcpd_reg <- pcpd(plot_data$observed, plot_data$predicted)
r_squared_reg <- RSQUARE(plot_data$observed, plot_data$predicted)

# Create a data frame with the results and display them
results_df <- data.frame(
  Model = c("Holt-Winters", "ARIMA", "Regression Analysis"),
  MAE = c(mae_hw, mae_arima, mae_reg),
  MSE = c(mse_hw, mse_arima, mse_reg),
  Forecast_Bias = c(bias_hw, bias_arima, bias_reg),
  PCPD = c(pcpd_hw, pcpd_arima, pcpd_reg),
  R_squared = c(r_squared_hw, r_squared_arima, r_squared_reg)
)

kable(results_df, caption = "Model Comparison Results", format = "markdown")





#aggregate to obtain the max and mean daily consumption
daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=function(x) {
  sorted_values <- sort(x, decreasing=TRUE)
  top_three_mean <- mean(sorted_values[1:3])
  c(max=top_three_mean, mean=mean(x))
}))
daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= "2012-06-01" & daily_dataset.tmp$day <= "2014-02-27",]
daily_dataset.tmp <- daily_dataset.tmp %>%
  mutate(
    x.mean = x[, "mean"],
    x.max = x[, "max"]
  ) 

# implementation of the 3 lag centered moving average
daily_dataset.tmp$x.max <- stats::filter(daily_dataset.tmp$x.max, rep(1/3, 3), sides = 2)
daily_dataset.tmp$x.mean <- stats::filter(daily_dataset.tmp$x.mean, rep(1/3, 3), sides = 2)

#plot the resulting dataset
plot(daily_dataset.tmp$day, daily_dataset.tmp$x.max, type="l", col="red", lty=1, ylim=c(0, 220), main="Daily Consumption Statistics", xlab="Day", ylab="Consumption")
lines(daily_dataset.tmp$day, daily_dataset.tmp$x.mean, col="blue", lty=1)
legend("topright", legend=c("Max", "Mean"), col=c("red", "blue"), lty=1:1)

# Prepare the data for multivariate regression
ts_data <- na.omit(daily_dataset.tmp) %>% dplyr::select(-day) %>% dplyr::select(-x)  # Exclude the 'day' column for regression





ts_data <- ts(ts_data, start = c(1, 1), frequency = 1)

# Using VARselect to choose the optimal p value
var_select <- VARselect(ts_data, lag.max = 120, type = "both")
#print(var_select)

var_model <- VAR(ts_data, p = 32)
var_forecast <- forecast(var_model, h = 30, level = 50)

plot(var_forecast, main="Forecast for mean and max consumption", xlim=c(200, 650))





#extracting CSV dataFrame for NN training in Python
daily_dataset.tmp <- daily_dataset.filtered
daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= "2012-01-01" & daily_dataset.tmp$day <= "2014-02-27",]
daily_dataset.tmp <- daily_dataset.tmp %>%
  mutate(
    year = as.numeric(strftime(day, format = "%Y")),
  )

stdorToU_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(stdorToU=daily_dataset.filtered$stdorToU), FUN=mean)
stdorToU_sorted <- stdorToU_sorted[order(stdorToU_sorted$x), ]$stdorToU

daily_dataset.tmp$weather_condition <- match(daily_dataset.tmp$weather_condition, weather_condition_sorted)
daily_dataset.tmp$precipType <- match(daily_dataset.tmp$precipType, precipType_sorted)
daily_dataset.tmp$stdorToU <- match(daily_dataset.tmp$stdorToU, stdorToU_sorted)

write.csv(daily_dataset.tmp, file = ".\\archive\\final_dataset.csv", row.names = FALSE)

