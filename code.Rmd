---
title: "Time Series Analysis and Forecasting Project"
author: "Feroldi Francesco"
---

# assumptions

This project is created to analyze the behaviour of a network of electricity consumers as a whole to understand the effect of heterogeneous variables on the consumption profile of the network.
To do so we will work under the assumptions that:

1) except for the first few months where clients where joining the network, the data is representative of the consumption of the whole network;
2) clients have no consumption outside of what is reported in the dataset;
3) missing values are due to communication issues between the smart meter and the operations center rather than due to no consumption from the client;
4) the trend identified through yearly seasonal decomposition is due to de variation in the consumption behaviour of clients (acquisition of greener technology such as solar panels, LED light bulbs, better water heaters) rather than the weather conditions;

# STEP 1: IMPORT AND ORGANIZE DATA FROM FS

```{r include=FALSE}
Sys.setlocale("LC_TIME", "C")
# packages to install
# install.packages("performanceEstimation")
# install.packages("fpp3")

# Load libraries
library(dplyr)
library(lubridate)
library(ggplot2)

# data sources
daily_dataset <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\daily_dataset.csv"
informations_households <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\informations_households.csv"
uk_bank_holidays <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\uk_bank_holidays.csv"
weather_daily_darksky <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\weather_daily_darksky.csv"

# import the csv file
daily_dataset <- read.csv(daily_dataset)
informations_households <- read.csv(informations_households)
uk_bank_holidays <- read.csv(uk_bank_holidays)
weather_daily_darksky <- read.csv(weather_daily_darksky)

# flag UK holiday days and add DOW and DOY columns, wile filtering for duplicate values
daily_dataset.augmented <- daily_dataset %>%
  mutate(
    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
    day = as.Date(day, format = "%Y-%m-%d"),
    dayOfWeek = wday(day, week_start=1),
    dayOfYear = as.numeric(strftime(day, format = "%j")),
    consumption = energy_sum
  )

# add sunlight hours column to weather_daily_darksky
weather_daily_darksky <- weather_daily_darksky %>%
  mutate(
    sunlightHours = difftime(sunsetTime, sunriseTime, units = "hours"),
    weather_condition = icon
  )

# Convert weather time column to date
weather_daily_darksky$day <- as.Date(weather_daily_darksky$time)
weather_daily_darksky <- weather_daily_darksky[!duplicated(weather_daily_darksky$day), ]

# Merge datasets and select only useful columns
daily_dataset.merged <- daily_dataset.augmented %>%
  inner_join(informations_households, by = "LCLid") %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  select(
    LCLid,
    day,
    consumption,
    isUkHoliday,
    dayOfWeek,
    dayOfYear,
    stdorToU,
    weather_condition,
    cloudCover,
    windBearing,
    windSpeed,
    pressure,
    visibility,
    humidity,
    precipType,
    uvIndex,
    sunlightHours,
    temperatureMin,
    temperatureMax
  )
```

# STEP 2: DATA EXPLORATION, DATA CLEANING AND DATA TRANSFORMATION

```{r fig.cap = "Daily Dataset density distribution", echo=FALSE}
library(ggplot2)

freq_table <- table(daily_dataset$day)
freq_data <- as.data.frame(table(daily_dataset$day))
freq_data$day <- as.Date(names(freq_table))

threshold <- 0.75 * max(freq_data$Freq)

ggplot(freq_data, aes(x = day, y = Freq)) +
  geom_line() +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red") +
  labs(title = "Daily Dataset Frequency Plot", x = "Day", y = "Frequency") +
  theme_minimal()
```

```{r include=FALSE}
# filtering values whose
daily_dataset.filtered <- daily_dataset.merged[daily_dataset.merged$day %in% freq_data$day[freq_data$Freq >= threshold], ]

unfiltered_data_points.mln <- round(nrow(daily_dataset.merged) / 1000000, 1)
filtered_data_points.mln <- round(nrow(daily_dataset.filtered) / 1000000, 1)
```
The filtering process reduced the overall number of data points from `r unfiltered_data_points.mln`mln to `r filtered_data_points.mln`mln .

```{r include=FALSE}
# identifying missing values

summary(daily_dataset.filtered)

consumption_missing_values <- sum(is.na(daily_dataset.filtered$consumption))
cloudCover_missing_values <- sum(is.na(daily_dataset.filtered$cloudCover))
uvIndex_missing_values <- sum(is.na(daily_dataset.filtered$uvIndex))

cc_uv_missing_values.dates <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$cloudCover) & is.na(daily_dataset.filtered$uvIndex), ]$day) # nolint: line_length_linter.
consumption_missing_values.dates.num <- length(unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$day)) # nolint: line_length_linter.

values_cc_uv_missing_values.dates <- sum(daily_dataset.filtered$day == as.Date(cc_uv_missing_values.dates))
```
A few null values where identified in three different columns:

* [`r consumption_missing_values`] from the Consumption column
* [`r cloudCover_missing_values`] from the Cloud Coverage index column
* [`r uvIndex_missing_values`] from the Ultraviolet index 

The first is given by a few missing values distributed in `r consumption_missing_values.dates.num` different days.
The other two variables however, have missing values representing the entirety of the values available for the `r format(cc_uv_missing_values.dates, format = "%dst of %B of %Y")` (as the values were missing from the original table "weather_daily_darksky" where each row represents the weather conditions for one day).

With this kind of dataset the use basic data imputation techniques can yield misleading results as a change in the weather conditions varies the value of the variables greatly.
The data imputation function which might be able to produce the best outcome is the K-nearest neighbors data imputation function.

```{r include=FALSE} 

library(performanceEstimation)
# cloudCover and uvIndex imputation of missing values

#imputate first the two values in the weather daily darskky table
weather_daily_darksky$sunlightHours_numeric <- as.numeric(weather_daily_darksky$sunlightHours)

features <- c("temperatureMax", "temperatureMin", "windBearing", "dewPoint", "windSpeed", "pressure", "visibility", "humidity", "sunlightHours_numeric", "x")

weather_daily_darksky$x <- as.numeric(weather_daily_darksky$uvIndex)
weather_daily_darksky$uvIndex <- knnImp(weather_daily_darksky[, features], k = 10)$x

weather_daily_darksky$x <- as.numeric(weather_daily_darksky$cloudCover)
weather_daily_darksky$cloudCover <- knnImp(weather_daily_darksky[, features], k = 10)$x

daily_dataset.filtered <- daily_dataset.filtered[, !(names(daily_dataset.filtered) %in% setdiff(colnames(weather_daily_darksky), "day"))]

daily_dataset.filtered <- daily_dataset.filtered %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  select(
    LCLid,
    day,
    consumption,
    isUkHoliday,
    dayOfWeek,
    dayOfYear,
    stdorToU,
    weather_condition,
    cloudCover,
    windBearing,
    windSpeed,
    pressure,
    visibility,
    humidity,
    precipType,
    uvIndex,
    sunlightHours,
    temperatureMin,
    temperatureMax,
    sunlightHours_numeric
  )

daily_dataset.filtered$sunlightHours <- as.numeric(daily_dataset.filtered$sunlightHours)

# Define the features for imputation
features <- c("dayOfYear", "dayOfWeek", "temperatureMax", "temperatureMin", "windBearing", "windSpeed", "pressure", "visibility", "humidity", "sunlightHours", "x", "uvIndex", "cloudCover")

# imputate only for the given user
LCLid.list <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$LCLid)

daily_dataset.filtered$x <- as.numeric(daily_dataset.filtered$consumption)

for(lcl in LCLid.list) {
  lcl_rows = daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ]
  readingsCount <- nrow(daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, ])
  if(readingsCount <= 50){
    # if the number of values is not sufficient we remove the null rows for the given client
    daily_dataset.filtered <- daily_dataset.filtered[!(daily_dataset.filtered$LCLid == lcl & is.na(daily_dataset.filtered$consumption)), ]
  }else{
    # otherwise we apply the imputation
    daily_dataset.filtered.temp <- daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]
    daily_dataset.filtered[daily_dataset.filtered$LCLid == lcl, features]$x <- knnImp(daily_dataset.filtered.temp, k = 10)$x
  }
}
daily_dataset.filtered$consumption <- daily_dataset.filtered$x
daily_dataset.filtered <- subset(daily_dataset.filtered, select = -x )

```

#STEP 3: EXPLORATORY ANALYSIS

For Ease of analisys we apply the seasonal decomposition on two exact years.
```{r echo=FALSE}

# analysis of trend and seasonality
daily_dataset.consumption <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))
daily_dataset.consumption <- daily_dataset.consumption[daily_dataset.consumption$day >= "2012-01-01" & daily_dataset.consumption$day <= "2014-01-01",]

ts_consumption.week <- ts(daily_dataset.consumption$x, frequency = 7)
ts_consumption.week.decom <- stl(ts_consumption.week,s.window="periodic")

plot( ts_consumption.week.decom , main="WEEKLY SEASONAL COMPONENTS DECOMPOSITION")

ts_consumption.year <- ts(na.omit(ts_consumption.week.decom$time.series[, "trend"]), frequency = 360)
ts_consumption.year.decom <- stl(ts_consumption.year,s.window="periodic")

plot( ts_consumption.year.decom , main="YEARLY SEASONAL COMPONENTS DECOMPOSITION" )

```
The graphs are the result of the seasonal decomposition of the time series to extrapolate the weekly components, then feed the weekly trend component back to the seasonal decomposition function and extrapolate the yearly components.
There are two immediately noticeable seasonal components with both yearly and weekly frequency. There also is a considerable amount of variance related to the randomicity of the weather conditions (emphasized from the fact that the dataset has been recorded in London).

If we extract only the random components of the time series we can see that the variance is not constant but it is more pronounced during a given period of the year (roughly during the winter)
```{r echo=FALSE}

ts_consumption.week.decom.remainder <- ts(na.omit(ts_consumption.week.decom$time.series[, "remainder"]), frequency = 360)
ts_consumption.week.decom.remainder <- ts(na.omit(ts_consumption.week.decom$time.series[, "remainder"]), frequency = 360)

plot( ts_consumption.week.decom.remainder + ts_consumption.week.decom.remainder, xlab="Time (in YEARS)", ylab="consumption variance", main="WEEKLY VARIANCE + YEARLY VARIANCE")

```
To better pinpoint the effect of the wether conditions on the time series we will remove the weekly seasonality, and yearly trend from the time series (to only keep the components previously identified as variance and those who are most likely influenced by the weather conditions). ([ASSUMPTION n°4](#assumptions))
```{r echo=FALSE}

ts_consumption.year.decom.seasonal <- ts(na.omit(ts_consumption.year.decom$time.series[, "seasonal"]), frequency = 360)

plot( ts_consumption.week.decom.remainder + ts_consumption.week.decom.remainder + ts_consumption.year.decom.seasonal, xlab="Time (in YEARS)", ylab="consumption variance", main="WEEKLY VARIANCE + YEARLY VARIANCE + YEARLY SEASONALITY")

```


```{r include = FALSE}
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)

# remove non weather influenced components from daily_dataset.consumption
daily_dataset.consumption$x <- daily_dataset.consumption$x - ts(na.omit(ts_consumption.week.decom$time.series[, "seasonal"]), frequency = 360) - ts(na.omit(ts_consumption.year.decom$time.series[, "trend"]), frequency = 360)

weather_daily_darksky <- subset(weather_daily_darksky, select = -x )

daily_dataset.consumption <- daily_dataset.consumption %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  select(
    day,
    x,
    weather_condition,
    cloudCover,
    windBearing,
    windSpeed,
    pressure,
    visibility,
    humidity,
    precipType,
    uvIndex,
    sunlightHours_numeric,
    temperatureMin,
    temperatureMax
  )

daily_dataset.consumption <- daily_dataset.consumption %>%
  mutate(
    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
    dayOfWeek = wday(day, week_start=1),
    dayOfYear = as.numeric(strftime(day, format = "%j")),
    consumption = x
  )

daily_dataset.consumption <- subset(daily_dataset.consumption, select = -x )
```
To allow the measurement of the correlation level, some character variables have been converted in numeric by replacing their value with the index of said value inside of a vector (containing all values sorted by mean consumption).
```{r message = FALSE}
weather_condition_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(weather_condition=daily_dataset.filtered$weather_condition), FUN=mean)
weather_condition_sorted <- weather_condition_sorted[order(weather_condition_sorted$x), ]$weather_condition

daily_dataset.consumption$weather_condition <- match(daily_dataset.consumption$weather_condition, weather_condition_sorted)

precipType_sorted <- aggregate(daily_dataset.filtered$consumption, by=list(precipType=daily_dataset.filtered$precipType), FUN=mean)
precipType_sorted <- precipType_sorted[order(precipType_sorted$x), ]$precipType

daily_dataset.consumption$precipType <- match(daily_dataset.consumption$precipType, precipType_sorted)
```

```{r echo = FALSE}
corr_matrix.pearson <- rcorr(as.matrix(daily_dataset.consumption[, c(-1, -14)]), type = "pearson")
corrplot(corr_matrix.pearson[[1]][1:14, 15, drop=FALSE], cl.pos='n', method = "number", mar=c(0,0,1,0) ,title = "Pearson correlation for consumption")

```
In the end the Pearson correlation coefficient has been chosen as it provided the correlation values with highest modulus for all variables.
As we can see, having removed the weekly component of seasonality, the "dayOfWeek" endogeneous variable has next to no correlation with the consumption target variable. There is also a few other variables such as "windBearing" and "pressure" that, due to the limited amount of correlation with the target variable, will have a limited effect when evaluating the effect of weather conditions on consumption, so they will be removed when implementing neural networks to the time series.

#STEP 4: APPLY FORECASTING TECHNIQUES (holt-winters, ARIMA)

```{r echo=FALSE}
# apply holt-winters forecasting technique
daily_dataset.tmp <- na.omit(aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean))
daily_dataset.tmp <- daily_dataset.tmp[daily_dataset.tmp$day >= "2012-01-01" & daily_dataset.tmp$day <= "2014-02-27",]

ts_consumption.month <- ts(daily_dataset.tmp$x, frequency = 360)
ts_consumption.month.HW <- HoltWinters(ts_consumption.month, seasonal = "multiplicative")

ts_consumption.month.HW.predict <- predict(ts_consumption.month.HW, 360, prediction.interval = TRUE, level=0.5)

plot(ts_consumption.month, ylab="consumption", xlim=c(1, 4.3), ylim=c(4, 16)) + lines(ts_consumption.month.HW$fitted[,1], lty=2, col="green") + lines(ts_consumption.month.HW.predict[,1], col="red") + lines(ts_consumption.month.HW.predict[,2], col="yellow") + lines(ts_consumption.month.HW.predict[,3], col="yellow")

```
very low confidence level (0.5) due to the high variance of the time series