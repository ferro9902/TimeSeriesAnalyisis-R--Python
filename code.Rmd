---
title: "Time Series Analysis and Forecasting Project"
author: "Feroldi Francesco"
---

# STEP 1: IMPORT AND ORGANIZE DATA FROM FS

```{r include=FALSE}
Sys.setlocale("LC_TIME", "C")
# packages to install
# install.packages("performanceEstimation")
# install.packages("fpp3")

# Load libraries
library(dplyr)
library(lubridate)
library(ggplot2)

# data sources
daily_dataset <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\daily_dataset.csv"
informations_households <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\informations_households.csv"
uk_bank_holidays <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\uk_bank_holidays.csv"
weather_daily_darksky <- "C:\\Users\\ferol\\Documents\\programming\\TimeSeriesAnalyisis R+ Python\\archive\\weather_daily_darksky.csv"

# import the csv file
daily_dataset <- read.csv(daily_dataset)
informations_households <- read.csv(informations_households)
uk_bank_holidays <- read.csv(uk_bank_holidays)
weather_daily_darksky <- read.csv(weather_daily_darksky)

# flag UK holiday days and add DOW and DOY columns, wile filtering for duplicate values
daily_dataset.augmented <- daily_dataset %>%
  mutate(
    isUkHoliday = as.numeric(day %in% uk_bank_holidays$Bank.holidays),
    day = as.Date(day, format = "%Y-%m-%d"),
    dayOfWeek = weekdays(day),
    dayOfYear = as.numeric(strftime(day, format = "%j")),
    consumption = energy_sum
  )

# add sunlight hours column to weather_daily_darksky
weather_daily_darksky <- weather_daily_darksky %>%
  mutate(
    sunlightHours = difftime(sunsetTime, sunriseTime, units = "hours"),
    weather_condition = icon
  )

# Convert weather time column to date
weather_daily_darksky$day <- as.Date(weather_daily_darksky$time)
weather_daily_darksky <- weather_daily_darksky[!duplicated(weather_daily_darksky$day), ]

# Merge datasets and select only useful columns
daily_dataset.merged <- daily_dataset.augmented %>%
  inner_join(informations_households, by = "LCLid") %>%
  inner_join(weather_daily_darksky, by = c("day" = "day")) %>%
  select(
    LCLid,
    day,
    consumption,
    isUkHoliday,
    dayOfWeek,
    dayOfYear,
    stdorToU,
    weather_condition,
    cloudCover,
    windSpeed,
    precipType,
    uvIndex,
    sunlightHours,
    temperatureMin,
    temperatureMax
  )
```

# STEP 2: DATA EXPLORATION, DATA CLEANING AND DATA TRANSFORMATION

```{r fig.cap = "Daily Dataset density distribution", echo=FALSE}
library(ggplot2)

freq_table <- table(daily_dataset$day)
freq_data <- as.data.frame(table(daily_dataset$day))
freq_data$day <- as.Date(names(freq_table))

threshold <- 0.75 * max(freq_data$Freq)

ggplot(freq_data, aes(x = day, y = Freq)) +
  geom_line() +
  geom_hline(yintercept = threshold, linetype = "dashed", color = "red") +
  labs(title = "Daily Dataset Frequency Plot", x = "Day", y = "Frequency") +
  theme_minimal()
```

```{r include=FALSE}
# filtering values whose
daily_dataset.filtered <- daily_dataset.merged[daily_dataset.merged$day %in% freq_data$day[freq_data$Freq >= threshold], ]

unfiltered_data_points.mln <- round(nrow(daily_dataset.merged) / 1000000, 1)
filtered_data_points.mln <- round(nrow(daily_dataset.filtered) / 1000000, 1)
```
The filtering process reduced the overall number of data points from `r unfiltered_data_points.mln`mln to `r filtered_data_points.mln`mln .

```{r include=FALSE}
# identifying missing values

summary(daily_dataset.filtered)

consumption_missing_values <- sum(is.na(daily_dataset.filtered$consumption))
cloudCover_missing_values <- sum(is.na(daily_dataset.filtered$cloudCover))
uvIndex_missing_values <- sum(is.na(daily_dataset.filtered$uvIndex))

cc_uv_missing_values.dates <- unique(daily_dataset.filtered[is.na(daily_dataset.filtered$cloudCover) & is.na(daily_dataset.filtered$uvIndex), ]$day) # nolint: line_length_linter.
consumption_missing_values.dates.num <- length(unique(daily_dataset.filtered[is.na(daily_dataset.filtered$consumption), ]$day)) # nolint: line_length_linter.

values_cc_uv_missing_values.dates <- sum(daily_dataset.filtered$day == as.Date(cc_uv_missing_values.dates))
```
A few null values where identified in three different columns:

* [`r consumption_missing_values`] from the Consumption column
* [`r cloudCover_missing_values`] from the Cloud Coverage index column
* [`r uvIndex_missing_values`] from the Ultraviolet index 

The first is given by a few missing values distributed in `r consumption_missing_values.dates.num` different days.
The other two variables however, have missing values representing the entirety of the values available for the `r format(cc_uv_missing_values.dates, format = "%dst of %B of %Y")` (as the values were missing from the original table "weather_daily_darksky" where each row represents the weather conditions for one day).

With this kind of dataset the use of most basic data imputation techniques can yield misleading results as a change in the weather conditions varies the value of the variables greatly.
The two data imputation functions which might be able to produce the best outcome are the K-nearest neighbors and miss-forest data imputation functions.

```{r include=FALSE} 

library(performanceEstimation)
# cloudCover and uvIndex imputation of missing values

#imputate first the two values in the weather daily darskky table
weather_daily_darksky$day_numeric <- as.numeric(weather_daily_darksky$day)
weather_daily_darksky$sunlightHours_numeric <- as.numeric(weather_daily_darksky$sunlightHours)

features <- c("temperatureMax", "temperatureMin", "windBearing", "dewPoint", "windSpeed", "pressure", "visibility", "humidity", "moonPhase", "sunlightHours_numeric", "day_numeric", "x")

weather_daily_darksky$x <- as.numeric(weather_daily_darksky$uvIndex)
weather_daily_darksky$uvIndex_imputed <- knnImp(weather_daily_darksky[, features], k = 5)$x

daily_dataset.filtered.noNa$cloudCover_imputed <- knn(weather_daily_darksky[, features], weather_daily_darksky$cloudCover, k = 5)

daily_dataset.filtered.noNa<-data.frame(daily_dataset.filtered)
daily_dataset.filtered.noNa$day_numeric <- as.numeric(daily_dataset.filtered.noNa$day)
daily_dataset.filtered.noNa <- na.omit(daily_dataset.filtered.noNa)

daily_dataset.filtered.noNa <- daily_dataset.filtered.noNa %>% arrange(LCLid, day)
daily_dataset.filtered.noNa <- daily_dataset.filtered.noNa %>% group_by(LCLid) %>% mutate(consumption_previous_day = lag(consumption))

# Define the features for imputation
features <- c("consumption_previous_day", "day_numeric", "dayOfWeek", "dayOfYear", "isUkHoliday", "temperatureMax", "temperatureMin", "uvIndex", "cloudCover")

#daily_dataset.filtered.noNa$consumption_imputed <- KNNimp(data, k = 10, scale = TRUE, meth = "weighAvg", distData = daily_dataset.filtered.noNa)
```

```{r include=FALSE}
library(fpp3)

# analysis of trend and seasonality
daily_dataset.consumption <- aggregate(daily_dataset.merged$consumption, by=list(day=daily_dataset.merged$day), FUN=mean)

ts_consumption.week <- ts(na.omit(daily_dataset.consumption[daily_dataset.consumption$day <= "2014-02-27",]$x), frequency = 7)
ts_consumption.week.decom <- stl(ts_consumption.week,s.window="periodic")

plot( ts_consumption.week.decom )

ts_consumption.year <- ts(na.omit(ts_consumption.week.decom$time.series[, "trend"]), frequency = 365)
ts_consumption.year.decom <- stl(ts_consumption.year,s.window="periodic")

plot( ts_consumption.year.decom )

```
There are two immediately noticeable seasonal components with both yearly and weekly frequency. There also is a considerable amount of variance related to the randomicity of the weather conditions (emphasized from the fact that the dataset has been recorded in London).